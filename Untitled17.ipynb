{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT4Od3tOVCYz1k93SdrWhM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rosireddy-V/testing_with_colab/blob/main/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWOILJvqK2Ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98492faa-b149-4ff6-a205-a0a3d81c282a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_sentences(data):\n",
        "  tokenize_sentence=[]\n",
        "  for sentence in data:\n",
        "    sentence=sentence.lower()\n",
        "    tokens=nltk.word_tokenize(sentence)\n",
        "    tokenize_sentence.append(tokens)\n",
        "  return tokenize_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "Ea90fSfDd2bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.tokenize import word_tokenize,sentence_tokenize\n",
        "\n",
        "text=[\"The sun sets over the horizon, painting the sky with hues of orange and pink.\",\"In the heart of the forest, a gentle breeze rustles the leaves.\",\"She walked along the sandy shore, feeling the cool water on her feet.\",\"The old bookshop on the corner is filled with stories waiting to be discovered.\",\"As the rain falls outside, I sit by the window with a cup of hot tea.\"]\n",
        "tokenized_sentences(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c88HxZScRJ4f",
        "outputId": "81c0dda4-a78a-472a-c2c5-ea5a13671981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the',\n",
              "  'sun',\n",
              "  'sets',\n",
              "  'over',\n",
              "  'the',\n",
              "  'horizon',\n",
              "  ',',\n",
              "  'painting',\n",
              "  'the',\n",
              "  'sky',\n",
              "  'with',\n",
              "  'hues',\n",
              "  'of',\n",
              "  'orange',\n",
              "  'and',\n",
              "  'pink',\n",
              "  '.'],\n",
              " ['in',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'the',\n",
              "  'forest',\n",
              "  ',',\n",
              "  'a',\n",
              "  'gentle',\n",
              "  'breeze',\n",
              "  'rustles',\n",
              "  'the',\n",
              "  'leaves',\n",
              "  '.'],\n",
              " ['she',\n",
              "  'walked',\n",
              "  'along',\n",
              "  'the',\n",
              "  'sandy',\n",
              "  'shore',\n",
              "  ',',\n",
              "  'feeling',\n",
              "  'the',\n",
              "  'cool',\n",
              "  'water',\n",
              "  'on',\n",
              "  'her',\n",
              "  'feet',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'old',\n",
              "  'bookshop',\n",
              "  'on',\n",
              "  'the',\n",
              "  'corner',\n",
              "  'is',\n",
              "  'filled',\n",
              "  'with',\n",
              "  'stories',\n",
              "  'waiting',\n",
              "  'to',\n",
              "  'be',\n",
              "  'discovered',\n",
              "  '.'],\n",
              " ['as',\n",
              "  'the',\n",
              "  'rain',\n",
              "  'falls',\n",
              "  'outside',\n",
              "  ',',\n",
              "  'i',\n",
              "  'sit',\n",
              "  'by',\n",
              "  'the',\n",
              "  'window',\n",
              "  'with',\n",
              "  'a',\n",
              "  'cup',\n",
              "  'of',\n",
              "  'hot',\n",
              "  'tea',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data=tokenized_sentences(text)\n",
        "train_size=int(len(tokenized_data)*0.8)\n",
        "train_data=tokenized_data[:train_size]\n",
        "test_data=tokenized_data[train_size:]"
      ],
      "metadata": {
        "id": "9gcS8VL8feE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{} data are split into {} training set and {} into test set\".format(len(tokenized_data),train_size,len(test_data)))\n",
        "print(\"first training sample\")\n",
        "print(train_data[0])\n",
        "print(\"first testing sample\")\n",
        "print(test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLHm_aKlimbN",
        "outputId": "350fb63d-9026-45d6-95a8-59be440dc28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 data are split into 4 training set and 1 into test set\n",
            "first training sample\n",
            "['the', 'sun', 'sets', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'with', 'hues', 'of', 'orange', 'and', 'pink', '.']\n",
            "first testing sample\n",
            "['as', 'the', 'rain', 'falls', 'outside', ',', 'i', 'sit', 'by', 'the', 'window', 'with', 'a', 'cup', 'of', 'hot', 'tea', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_datas):\n",
        "  word_count={}\n",
        "\n",
        "  for tokenized_words in tokenized_datas:\n",
        "    for word in tokenized_words:\n",
        "      if word in word_count.keys():\n",
        "        word_count[word]+=1\n",
        "      else:\n",
        "        word_count[word]=1\n",
        "  return word_count"
      ],
      "metadata": {
        "id": "VqfnlXtUjrHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab(tokenized_datas):\n",
        "  vocab=[]\n",
        "  for tokenized_words in tokenized_data:\n",
        "    for word in tokenized_words:\n",
        "      if word in vocab:\n",
        "        continue\n",
        "      else:\n",
        "        vocab.append(word)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "lJXBsU-mtGoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_words=count_words(train_data)\n",
        "vocabulary=vocab(train_data)\n",
        "print(no_of_words)\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_m9nNz0ljF8",
        "outputId": "603c919d-1cd4-40d8-86a4-de1e9a1dda9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 10, 'sun': 1, 'sets': 1, 'over': 1, 'horizon': 1, ',': 3, 'painting': 1, 'sky': 1, 'with': 2, 'hues': 1, 'of': 2, 'orange': 1, 'and': 1, 'pink': 1, '.': 4, 'in': 1, 'heart': 1, 'forest': 1, 'a': 1, 'gentle': 1, 'breeze': 1, 'rustles': 1, 'leaves': 1, 'she': 1, 'walked': 1, 'along': 1, 'sandy': 1, 'shore': 1, 'feeling': 1, 'cool': 1, 'water': 1, 'on': 2, 'her': 1, 'feet': 1, 'old': 1, 'bookshop': 1, 'corner': 1, 'is': 1, 'filled': 1, 'stories': 1, 'waiting': 1, 'to': 1, 'be': 1, 'discovered': 1}\n",
            "['the', 'sun', 'sets', 'over', 'horizon', ',', 'painting', 'sky', 'with', 'hues', 'of', 'orange', 'and', 'pink', '.', 'in', 'heart', 'forest', 'a', 'gentle', 'breeze', 'rustles', 'leaves', 'she', 'walked', 'along', 'sandy', 'shore', 'feeling', 'cool', 'water', 'on', 'her', 'feet', 'old', 'bookshop', 'corner', 'is', 'filled', 'stories', 'waiting', 'to', 'be', 'discovered', 'as', 'rain', 'falls', 'outside', 'i', 'sit', 'by', 'window', 'cup', 'hot', 'tea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ngrams(data,n,start_token='<s>',end_token='<e>'):\n",
        "  n_grams={}\n",
        "  for sentence in data:\n",
        "    sentence=[start_token]*n+sentence+[end_token]\n",
        "    sentence=tuple(sentence)\n",
        "    for i in range(len(sentence) if n==1 else len(sentence)-n+1):\n",
        "      n_gram=sentence[i:i+n]\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram]+=1\n",
        "      else:\n",
        "        n_grams[n_gram]=1\n",
        "  return n_grams"
      ],
      "metadata": {
        "id": "yzO_xHIYnUzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram=count_ngrams(train_data,1)\n",
        "bigram=count_ngrams(train_data,2)\n",
        "print(unigram)\n",
        "print(bigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ORY4lf5p4Fk",
        "outputId": "910a51ea-0a98-468e-b381-4690ec386ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('<s>',): 4, ('the',): 10, ('sun',): 1, ('sets',): 1, ('over',): 1, ('horizon',): 1, (',',): 3, ('painting',): 1, ('sky',): 1, ('with',): 2, ('hues',): 1, ('of',): 2, ('orange',): 1, ('and',): 1, ('pink',): 1, ('.',): 4, ('<e>',): 4, ('in',): 1, ('heart',): 1, ('forest',): 1, ('a',): 1, ('gentle',): 1, ('breeze',): 1, ('rustles',): 1, ('leaves',): 1, ('she',): 1, ('walked',): 1, ('along',): 1, ('sandy',): 1, ('shore',): 1, ('feeling',): 1, ('cool',): 1, ('water',): 1, ('on',): 2, ('her',): 1, ('feet',): 1, ('old',): 1, ('bookshop',): 1, ('corner',): 1, ('is',): 1, ('filled',): 1, ('stories',): 1, ('waiting',): 1, ('to',): 1, ('be',): 1, ('discovered',): 1}\n",
            "{('<s>', '<s>'): 4, ('<s>', 'the'): 2, ('the', 'sun'): 1, ('sun', 'sets'): 1, ('sets', 'over'): 1, ('over', 'the'): 1, ('the', 'horizon'): 1, ('horizon', ','): 1, (',', 'painting'): 1, ('painting', 'the'): 1, ('the', 'sky'): 1, ('sky', 'with'): 1, ('with', 'hues'): 1, ('hues', 'of'): 1, ('of', 'orange'): 1, ('orange', 'and'): 1, ('and', 'pink'): 1, ('pink', '.'): 1, ('.', '<e>'): 4, ('<s>', 'in'): 1, ('in', 'the'): 1, ('the', 'heart'): 1, ('heart', 'of'): 1, ('of', 'the'): 1, ('the', 'forest'): 1, ('forest', ','): 1, (',', 'a'): 1, ('a', 'gentle'): 1, ('gentle', 'breeze'): 1, ('breeze', 'rustles'): 1, ('rustles', 'the'): 1, ('the', 'leaves'): 1, ('leaves', '.'): 1, ('<s>', 'she'): 1, ('she', 'walked'): 1, ('walked', 'along'): 1, ('along', 'the'): 1, ('the', 'sandy'): 1, ('sandy', 'shore'): 1, ('shore', ','): 1, (',', 'feeling'): 1, ('feeling', 'the'): 1, ('the', 'cool'): 1, ('cool', 'water'): 1, ('water', 'on'): 1, ('on', 'her'): 1, ('her', 'feet'): 1, ('feet', '.'): 1, ('the', 'old'): 1, ('old', 'bookshop'): 1, ('bookshop', 'on'): 1, ('on', 'the'): 1, ('the', 'corner'): 1, ('corner', 'is'): 1, ('is', 'filled'): 1, ('filled', 'with'): 1, ('with', 'stories'): 1, ('stories', 'waiting'): 1, ('waiting', 'to'): 1, ('to', 'be'): 1, ('be', 'discovered'): 1, ('discovered', '.'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def est_prob(word,prev_word,n_gram_counts,n_plus1_gram_counts,v,k=1):\n",
        "  prev_ngram=tuple(prev_word)\n",
        "  prev_ngram_count=n_gram_counts[prev_ngram] if prev_ngram in n_gram_counts else 0\n",
        "  denominator=prev_ngram_count+k*v\n",
        "  prev_nplus1gram_count=prev_ngram+(word,)\n",
        "  prev_nplus1_gram_count=n_plus1_gram_counts[prev_nplus1gram_count] if prev_nplus1gram_count in n_plus1_gram_counts else 0\n",
        "  numerator=prev_nplus1_gram_count+k\n",
        "  probability=numerator/denominator\n",
        "  return probability\n"
      ],
      "metadata": {
        "id": "kr_UJxbEsEYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=len(vocabulary)\n",
        "prob=est_prob(\"falls\",[\"rain\"],unigram,bigram,s,k=1)\n",
        "print(f\"the estimated probability of falls after rain is: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMbO20Zvmfh",
        "outputId": "ee27cb86-854b-412a-d433-d651456edaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the estimated probability of falls after rain is: 0.0182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words=len(list(set(test_data)))"
      ],
      "metadata": {
        "id": "OeHUdciXw19J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_char_del=[]\n",
        "for tup in splits:\n",
        "    if tup[1]:\n",
        "        one_char_del.append([tup[0]+tup[1][1:]])\n",
        "one_char_del"
      ],
      "metadata": {
        "id": "DmVLqUp4kg8N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "44479f38-ad5c-4cb7-ccef-41e71caf1e8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d51754a7fc94>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mone_char_del\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mone_char_del\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mone_char_del\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'splits' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQrD0KKf9L86"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}